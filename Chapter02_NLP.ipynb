{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About this file\n",
    "\n",
    "This is the sentiment140 dataset.\n",
    "It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 2 = neutral, 4 = positive) and they can be used to detect sentiment .\n",
    "It contains the following 6 fields:\n",
    "\n",
    "    target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "    ids: The id of the tweet ( 2087)\n",
    "    date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "    flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "    user: the user that tweeted (robotickilldozr)\n",
    "    text: the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tmp = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding='latin-1', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_tmp.values, columns=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][:10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only a subset of the data\n",
    "nb_lines = 1000\n",
    "\n",
    "extract = pd.concat([df[df['target'] == 0][:nb_lines], \n",
    "                     df[df['target'] == 2][:nb_lines],\n",
    "                     df[df['target'] == 4][:nb_lines]])\n",
    "\n",
    "extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with no prior treament\n",
    "\n",
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(extract['text'].values)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe:\n",
    "- @username, that we can remove\n",
    "- Useless numbers, i.e. \n",
    "- Abbreviations like 2mmorow\n",
    "- Repeatited characters like aaaaawwww\n",
    "\n",
    "In my sample of 2000 tweets I got a vocabulary of ~5700 words, our goal is to reduce it to what is actually useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove @ username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(x):\n",
    "    # Process sentence\n",
    "    x = x.lower()\n",
    "    new_sentence = []\n",
    "    # Process each word of the sentence\n",
    "    for word in x.split():\n",
    "        # Remove @\n",
    "        if word[0] == '@':\n",
    "            continue\n",
    "        new_sentence.append(word)\n",
    "    return ' '.join(new_sentence)\n",
    "    \n",
    "extract['text'].apply(process_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove URLs and punctuation, but keep emojis\n",
    "\n",
    "Note : we keep the same `process_tweet` function and just add more fonctionnalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = {\n",
    "    \";D\": \"PositiveSmiley\",\n",
    "    \";-D\": \"PositiveSmiley\",\n",
    "    \":D\": \"PositiveSmiley\",\n",
    "    \":-D\": \"PositiveSmiley\",\n",
    "    \"xD\": \"PositiveSmiley\",\n",
    "    \":)\": \"PositiveSmiley\",\n",
    "    \":')\": \"PositiveSmiley\",\n",
    "    \":-)\": \"PositiveSmiley\",\n",
    "    \"D:\": \"NegativeSmiley\",\n",
    "    \":(\": \"NegativeSmiley\",\n",
    "    \":-(\": \"NegativeSmiley\",\n",
    "    \":'('\": \"NegativeSmiley\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_tweet(x):\n",
    "    # Process emojis\n",
    "    for emoji in emoji_dict.keys():\n",
    "        x = x.replace(emoji, emoji_dict[emoji])\n",
    "    # Process sentence\n",
    "    x = x.lower()\n",
    "    \n",
    "    new_sentence = []\n",
    "    # Process each word of the sentence\n",
    "    for word in x.split():\n",
    "        # Remove @\n",
    "        if word[0] == '@':\n",
    "            continue\n",
    "        # Remove URLs\n",
    "        if len(word.split(\"http\")) > 1:\n",
    "            continue\n",
    "        new_sentence.append(word)\n",
    "    x = ' '.join(new_sentence)\n",
    "    # Remove punctuations\n",
    "    x = re.sub(r'[^\\w\\s]', \"\", x)\n",
    "    return x\n",
    "\n",
    "print(extract['text'].values)\n",
    "extract['text'].apply(process_tweet).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove repeating characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_multiple_chars(word):\n",
    "    new_word = \"\"\n",
    "    same_occurence = 0\n",
    "    last_char = ''\n",
    "    for c in word:\n",
    "        if last_char and last_char == c:\n",
    "            same_occurence += 1\n",
    "            if same_occurence > 1:\n",
    "                continue\n",
    "        else:\n",
    "            last_char = c\n",
    "            same_occurence = 0\n",
    "        new_word += c\n",
    "        \n",
    "    return new_word\n",
    "\n",
    "print(rm_multiple_chars(\"aaawwwww\"))\n",
    "print(rm_multiple_chars(\"aww\"))\n",
    "print(rm_multiple_chars(\"aaaaaarrrrrrgggggg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(x):\n",
    "    # Process emojis\n",
    "    for emoji in emoji_dict.keys():\n",
    "        x = x.replace(emoji, emoji_dict[emoji])\n",
    "    # Process sentence\n",
    "    x = x.lower()\n",
    "    \n",
    "    new_sentence = []\n",
    "    # Process each word of the sentence\n",
    "    for word in x.split():\n",
    "        if word[0] == '@':\n",
    "            continue\n",
    "        if len(word.split(\"http\")) > 1:\n",
    "            continue\n",
    "        new_sentence.append(rm_multiple_chars(word))\n",
    "    x = ' '.join(new_sentence)\n",
    "    x = re.sub(r'[^\\w\\s]', \"\", x)\n",
    "    return x\n",
    "\n",
    "print(extract['text'].values)\n",
    "extract['text'].apply(process_tweet).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "lemmatizer.lemmatize(\"rocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatized has to be initialized before calling 'process_tweet'\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def process_tweet(x):\n",
    "    # Process emojis\n",
    "    for emoji in emoji_dict.keys():\n",
    "        x = x.replace(emoji, emoji_dict[emoji])\n",
    "    x = x.lower()\n",
    "    \n",
    "    # Process each word of the sentence\n",
    "    new_sentence = []\n",
    "    for word in x.split():\n",
    "        if word[0] == '@':\n",
    "            continue\n",
    "        if len(word.split(\"http\")) > 1:\n",
    "            continue\n",
    "        word = rm_multiple_chars(word)\n",
    "        # Lemmatize\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        new_sentence.append(word)\n",
    "    x = ' '.join(new_sentence)\n",
    "    \n",
    "    # Remove punctuations and digits\n",
    "    x = re.sub(r'[^\\w\\s]', \"\", x)\n",
    "    x = re.sub(r'[0-9]+', \"\", x)\n",
    "    return x\n",
    "\n",
    "print(extract['text'][:10].values)\n",
    "print(extract['text'][:10].apply(process_tweet).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_extract = extract['text'].apply(process_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(processed_extract.values)\n",
    "print(X.shape)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = extract['target'].values.tolist()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "\n",
    "print(accuracy_score(y_test, clf.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
